\chapter{AWS Implementation of Reinforcement Learning}

With DeepRacer, Amazon has created their own implementation of reinforcement learning, geared towards developers, with focus on learning about machine learning concepts. Compared to other robotics competitions, the DeepRacer League is more proprietary, as it is not just intended as a way of leaning about Machine Learning, but also a way for Amazon to promote its services. As a result, Amazon has tailored the reinforcement learning environment to suit their services. 

\section{Concept}
Reinforcement learning as a whole is a complex topic that can seem overwhelming. In order to simplify the learning process and create an even playing field for all participants, only some parts of the entire learning process can be modified. When training via the AWS cloud, Amazon allows only the configuration of certain hyperparameters, the action space and the creation of a custom reward function. This reward function, whose purpose was explained earlier, represents the core of their implementation. It has by far the largest impact on the performance of the car both in simulations and real world scenarios. Apart from those two modifications, Amazon offers very little in terms of customisation. This simplification has multiple reasons, the first one being that, as mentioned earlier, it offers programmers with less experience in reinforcement learning the option to participate in the races. Amazon themselves label the DeepRacer as a way to learn about machine learning.The second reason behind this simplification is that it provides a common basis for the races, which, like in any sport, is required in order to provide a fair competition.

\section{Reward Function}
The reward function determines the reward the agent receives for taking a certain action. Therefor the reward has to depend on the environment and the situation in which the agent is currently in, which is called the state. As it is not possible to capture all aspects of a situation, the actual state is abstracted into a predefined set of parameters. These parameters are the foundation which the entire reward process is based upon. 

The function itself is written in python and has to return a floating point number, which will from here on be called reward. The reward represents the immediate feedback the agent receives when moving along the track. This feedback acts as an incentive plan an what action the agent will take. If an incentive plan is not considered carefully, it can lead to unintended consequences of opposite effect\footcite{AWS19}. This is the case because an immediate reward alone is not enough to determine if an action has a positive effect, as this affect can come up delayed. This requires not only the current action but also subsequent actions to earn a positive reward for the agent in order for it to consider the initial action preferable. One example of such unintended consequences of opposite effect occurred during one of the first training sessions. Using a simple reward function which took only the speed and offset from the centre line into consideration, the agent began to drive in a zig-zag pattern on the centre line. The reason behind this was the reward function and how it gave a significant reward for staying close to the centre of the track. Following this concept, the agent maximised the received reward by diving in said zig-zag pattern. However due to this behaviour the vehicle performed poorly in terms of speed and was prone to going off track, especially when tested on a physical track in a real world environment.